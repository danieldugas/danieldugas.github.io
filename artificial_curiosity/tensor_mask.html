<!DOCTYPE html>
<html>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PP4068J99C"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PP4068J99C');
</script>
<!-- end of GA -->
<title>TensorMask, for You and I</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="../styles.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" integrity="sha384-fnmOCqbTlWIlj8LyTjo7mOUStjsKC4pOpQbqyi7RrhN7udi9RwhKkMHpvLbHG9Sr" crossorigin="anonymous">
<style>
body,h1 {font-family: "Raleway", Arial, sans-serif}
h1 {letter-spacing: 6px}
.w3-row-padding img {margin-bottom: 12px}
</style>
<body>

<!-- !PAGE CONTENT! -->
<div class="w3-content" style="max-width:1500px">

<!-- Header -->
<header class="w3-panel w3-center w3-opacity" style="padding:28px 16px">
  <h1 class="w3-xlarge">HOW DEEP IS THE MACHINE?</h1>
  <h1>The Artificial Curiosity Series</h1>

  <div class="w3-padding-32">
    <div class="w3-bar w3-border">
      <a href="../index.html" class="w3-bar-item w3-button">dugas.ch</a>
      <a href="../blog.html" class="w3-bar-item w3-button">/ Blog</a>
      <a href="./index.html" class="w3-bar-item w3-button">/ Artificial Curiosity</a>
      <a href="#" class="w3-bar-item w3-button w3-light-grey">/ TensorMask, for You and I</a>
    </div>
  </div>
</header>

<!-- Photo Grid -->
<div class="w3-row-padding" style="margin-bottom:128px;max-width:1000px">
  <h1>TensorMask, for You and I</h1>
  <p style="font-size:130%;">
TensorMask is the name for an image segmentation method, released in 2019 by its authors from FAIR. This is my attempt at understanding this paper.
  </p>
  <p style="font-size:130%;">

At the heart of the TensorMask paper is a question: could there be a better way to generate dense pixel masks than the current state of the art?

  </p>
  <p style="font-size:130%;">
For context, the most famous instance segmentation methods are probably RCNN and YOLO.
The difficult thing about masks is that unlike images or pixel classes, which have a fixed size, the amount and sizes of masks a model should output are variable.

  </p>
  <p style="font-size:130%;">
FIGURE - input image, several masks

  </p>
  <p style="font-size:130%;">
I think other articles do a good job of outlining how RCNN and YOLO get their bounding boxes (https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e), so I won’t go too much into it here.

  </p>
  <p style="font-size:130%;">
But the point is, wouldn’t it be nice if we could just generate dense masks in some fixed-shape tensor?
  </p>
  <p style="font-size:130%;">

    <h2> Sliding Window for Instance Segmentation </h2>

The core idea of TensorMask is relatively simple:
  </p>
  <p style="font-size:130%;">

At each pixel of the image, output a UxV mask which determines whether nearby pixels are in or out of the mask.
  </p>
  <p style="font-size:130%;">

FIGURE - dense mask at each pixel
  </p>
  <p style="font-size:130%;">

A simple idea, yes, but the execution is anything but: the authors put in a lot of effort in making it work in practice.
  </p>
  <p style="font-size:130%;">

I think it helps to start with the simple, core concept, and then gradually add complexity.

  </p>
  <p style="font-size:130%;">

Core concept - output a sliding-window tensor (U, V, H, W). In other words for each pixel in the image, our tensor contains a mask of size (U, V)
  </p>
  <p style="font-size:130%;">

    <h3> What is the mask? </h3>
You can think of a mask as a little window centered around some pixel. This window is a UxV grid of 0-1 values. 
Each value is a single probability, for example one if the pixel is part of a region, 0 otherwise.
  </p>
  <p style="font-size:130%;">

If there’s some values above a threshold (e.g. 0.5 https://github.com/CaoWGG/TensorMask/blob/master/models/detector.py#L148 ) in this window, then there is a segmentation mask for that pixel. If none of the values are above the threshold, then there is simply no segmentation mask there.
  </p>
  <p style="font-size:130%;">

    <h3>Ground truth data?</h3>
Authors take known masks from labelled datasets, and turn them into ground truth tensors. For example, if the ground truth label has a single mask with a bounding box centered at the pixel (5, 6), the 2d subtensor (U, V) which is a subset of the full tensor (U, V, H, W) at h=5, w=6
  </p>
  <p style="font-size:130%;">

In numpy you would write
  </p>
  <p style="font-size:130%;">

U = 15 # constants for the mask size
V = 15
tensormask[:U, :V, 5, 6] = label_mask[:U, :V]
  </p>
  <p style="font-size:130%;">

    <h3>Architecture?</h3>
Up to now, we just focus on inputs and outputs. But how do we actually produce this large (U, V, H, W) tensor from an input image e.g. (3, H, W)?
  </p>
  <p style="font-size:130%;">

You’d probably expect the answer to be ‘with some kind of neural network’...
and you’d be right.
  </p>
  <p style="font-size:130%;">

First, the author pass the image through a well known vision backbone called a Feature Pyramid Network (FPN), which takes as input a (3, H, W) image and produces a (C, H, W) feature tensor.
  </p>
  <p style="font-size:130%;">

Well, actually, it produces various feature tensors at different resolution levels (C, H/2, W/2), …, but we’ll discuss those a little further down. For now, we focus on the highest resolution output, which is (C, H, W).
  </p>
  <p style="font-size:130%;">

This feature tensor is passed through a single cnn layer outputing a (UxV, H, W) tensor which is reshaped to (U, V, H, W)
  </p>
  <p style="font-size:130%;">

Note: I was surprised by this because it means that the <b>mask information a few pixels away from a certain pixel is very tied to the features at that pixel</b>. Remember this fact, as it will be relevant when we talk about improvements that the authors tried.
  </p>
  <p style="font-size:130%;">

    <h3>Things get Complicated</h3>
So that’s the theory, but what about practice? Could this simple concept actually work?
  </p>
  <p style="font-size:130%;">

Well, reading the paper, it becomes clear that the authors had to overcome a few hurdles before this idea could actually produce useful results.
  </p>
  <p style="font-size:130%;">

    <h3>First Problem - tensor size:</h3>
  </p>
  <p style="font-size:130%;">

In the example picture above, one of the outputted masks takes up almost the entire picture. With this tensor idea, that would mean that we'd need to make U and V as large as H and W, which would make the tensors arbitrarily large - for a 600x600 pixel image, the full tensor would be 129’600’000’000 values, (129Gb if values are only zeros and ones).
  </p>
  <p style="font-size:130%;">

The first, most obvious way to reduce this size is to make masks smaller (smaller U, V). But this also means that large objects can't be segmented anymore. So the authors try a few other typical ways to make things smaller.
  </p>
  <p style="font-size:130%;">

For example, by playing with the stride, size and resolution of the masks themselves
  </p>
  <p style="font-size:130%;">

    <h3>HW stride</h3>
  </p>
  <p style="font-size:130%;">

For example, instead of outputting a UxV dense mask at every pixel, we could only output a mask every 10 pixels, which would make the tensor 100 times smaller (U, V, H/10, W/10)
  </p>
  <p style="font-size:130%;">

This is good, but doesn't take us all the way there, and if we push this too far, we lose the ability to get masks for small objects at all.
  </p>
  <p style="font-size:130%;">

FIGURE - strided mask 
  </p>
  <p style="font-size:130%;">

    <h3>UV Stride</h3>
  </p>
  <p style="font-size:130%;">

Another way we can save space is by making the masks themselves sparse. For example, we could produce a mask for each pixel in the image, but make that mask less dense by ‘spreading it out’ over more pixels. That’s another way to make the tensor smaller (U/4, V/4, H, W)
  </p>
  <p style="font-size:130%;">

FIGURE - low res mask
  </p>
  <p style="font-size:130%;">

    <h3>Upscaling Masks</h3>
  </p>
  <p style="font-size:130%;">

What about the gaps in the mask?
They can then be filled by upsampling.
  </p>
  <p style="font-size:130%;">

The ability to play with this striding is great, as it makes it feasible to output reasonably-sized tensors. 
  </p>
  <p style="font-size:130%;">

But everything has a cost: Almost all results point toward the notion that the more upsampling is used here, the more the model becomes ‘sensitive’ as it requires ‘tricks’ to stay effective (such as aligned representations - mentioned below, and bilinear upsampling)
  </p>
  <p style="font-size:130%;">

Also, unfortunately, these sparse mask don’t solve everything, as there is no single mask striding and size that can produce good masks for any kind of object in the image, small and big.
  </p>
  <p style="font-size:130%;">

FIGURE - small object - small, high res mask; big object, large, low-res mask
  </p>
  <p style="font-size:130%;">

    <h3>More heads</h3>
  </p>
  <p style="font-size:130%;">

Instead, the authors use a multi-resolution approach:
They make multiple copies of this tensor mask method, with various striding and size values. Each copy is called a ‘head’. Each head outputs a mask tensor, which can simply be combined (the same pixel could end up with multiple positive masks, I guess? But that sounds like it’s not a big issue)
  </p>
  <p style="font-size:130%;">

I found this not so intuitive, because it adds a bunch of parameters to the method - how many heads do you use? What HW striding do you use for each one? What UV striding and UV size?
In the end, this part of the method is very likely about optimization -
It would have been nice if we could simply hone in on a single tensor shape which works for all kinds of objects, but that would likely have been much too inefficient.
  </p>
  <p style="font-size:130%;">

Note: Would it have worked though? I would certainly be interested in finding out if the method still works the same with large, catch-all high-resolution masks - despite being much slower and taking much more memory.
  </p>
  <p style="font-size:130%;">

    <h3>Head parameters</h3>
  </p>
  <p style="font-size:130%;">

The authors do try several sets of head parameters and compare them.
Long story short, they find that they get their best results when they make multiple heads, with increasing H, W size and <i>proportionally decreasing U, V size</i>.
A neat thing is that it keeps the tensor size constant for all heads - a less neat thing is that since each head uses a different level of the FPN backend, which all have different HW resolutions, the inputs need to be upscaled before the pyramid heads. Oh well.
  </p>
  <p style="font-size:130%;">

	FIGURE - pyramid head - no, not this one
  </p>
  <p style="font-size:130%;">


    <h3>Idea - Aligned Sliding Window</h3>
Higher in this post, I mention how inefficient it seems to be, that the features for a given pixel contribute to mask information a bunch of pixels away. But the authors are like ‘have no fear, sweet summer child, for we have designed manmade wonders beyond your comprehension’, and present us with what they call the ‘Aligned Representations’. Basically, they shift things around in the (U, V, H, W) tensor, so that the tensor value at some pixel (x, y) is now the mask information of <b>this pixel</b> but for a <b>mask window centered at another pixel</b> which is u, v pixels away.
  </p>
  <p style="font-size:130%;">

Their figure can take a little bit of time to understand, but explains it better than I could.
  </p>
  <p style="font-size:130%;">

FIGURE from the paper, Aligned Representation
  </p>
  <p style="font-size:130%;">

Anyways, you can still quite well understand the method without fully understanding this - what really matters here is that this is a shifting around of values in the tensor, so it can be done easily and reversed, and does not change the tensor size. When the tensor is in ‘Aligned mode’ the neural network layers do a better job of computing useful local information. And when the tensor is put back into ‘Unaligned or natural mode’, it is simpler to understand for us, and easier to manipulate at the mask level.
  </p>
  <p style="font-size:130%;">

The authors show that this aligned representation makes a big difference when creating sparse masks that are then upscaled - and much less difference when using no up-scaling. (I find this surprising, as I would have expected the difference to be large without up-scaling still. Maybe it’s because they only use relatively small UV sizes in testing, which means things are never so far away? If that’s the case, I would bet that repeating the same test with non-upscaled, large-UV masks would lead to a greater impact of representation alignment.)
  </p>
  <p style="font-size:130%;">



    <h3>Remaining Questions:</h3>
  </p>
  <p style="font-size:130%;">

What happens to nearby, similar masks?
My understanding is that the model learns not to predict those because the dataset makes sure to exclude them. Still, I need to look at the code again to better understand how the authors avoid a failure case where the model outputs too many masks close to each other.
  </p>
  <p style="font-size:130%;">

    <h3>Fin?</h3>

After all these optimizations and improvements, the authors show that the tensormask model is competitive with Mask RCNN, which does seem to answer their initial question: Yes, this is a viable method for generating masks (if not a simple one, after all the upscaling, striding, aligning, multiple heads, and other tricks), using only fixed-size tensors and neural networks.
Will it be one of the classic image segmentation methods, in a few years? I guess we’ll all find out.
  </p>
  <p style="font-size:130%;">

With this basic understanding in mind, you might want to look a the the full paper which is available in open access (https://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_TensorMask_A_Foundation_for_Dense_Object_Segmentation_ICCV_2019_paper.pdf), or the code which the authors have made available on github (https://github.com/CaoWGG/TensorMask/tree/master). 
  </p>
  <p style="font-size:130%;">
  </p>
<!-- End Page Content -->
</div>


<!-- Javascript -->
<style>
.content {
  padding: 0 18px;
  background-color: white;
  max-height: 0;
  overflow: hidden;
  transition: max-height 0.2s ease-out;
}
</style>
<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {
  coll[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var content = this.nextElementSibling;
    if (content.style.maxHeight){
      content.style.maxHeight = null;
    } else {
      content.style.maxHeight = content.scrollHeight + "px";
    }
  });
}
</script>

<!-- Footer -->
<footer class="w3-container w3-padding-64 w3-light-grey w3-center w3-large">
  <!-- Font awesome logos: fa fa-something-something -->
  <i class="fab fa-pied-piper-alt w3-hover-opacity"></i>
  <p>Powered by <a  id="powered_by_link" href="http://beesbeesbees.com/" target="_blank" class="w3-hover-text-green">bees</a></p>

  <script>
  var powerSources = [
    "bees",
    "eels",
    "koalas",
    "circuits",
    "logic",
    "a chihuaha",
    "infinite zoom",
    ];
  var powerLinks = [
    "http://beesbeesbees.com/",
    "http://eelslap.com",
    "http://koalastothemax.com",
    "http://electricboogiewoogie.com",
    "http://www.visual6502.org/JSSim/index.html",
    "http://chihuahuaspin.com/",
    "http://zoomquilt.org/",
    ];
  var randomIndex = Math.floor(Math.random()*powerSources.length);
  document.getElementById("powered_by_link").href = powerLinks[randomIndex];
  document.getElementById("powered_by_link").innerHTML = powerSources[randomIndex];
  </script>
</footer>

</body>
</html>
